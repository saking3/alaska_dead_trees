{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c051036-bf63-4bfc-b536-25f61bbe8bd1",
   "metadata": {},
   "source": [
    "This notebook is for running predictions on an entire image (rather than just non-empty tiles) and stitching them together into a single image of predictions using the SAM Model. You will need a saved, trained SAM model to load in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03c28577-7fb5-4efb-a2ac-b3f7baea6f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sking11/.conda/envs/dino_env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import SamProcessor, SamModel, SamConfig\n",
    "import torch\n",
    "from patchify import patchify\n",
    "from torchvision import transforms\n",
    "from torch.optim import AdamW\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c2991d-1fba-494f-b8f2-55219e39c9f8",
   "metadata": {},
   "source": [
    "# Loading and Preparing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6bb24a-e9b9-42e5-a603-bb1bbdb060e2",
   "metadata": {},
   "source": [
    "Load in the image you would like to perform predictions on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3edf25b8-d38b-4466-91ca-ff6b6c4e33a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset\n",
    "image_paths_val = ['/explore/nobackup/people/sking11/MakingDinoDataset/gliht_1969_rgbi.tif']\n",
    "label_paths_val = ['/explore/nobackup/people/sking11/MakingDinoDataset/binarymasks/cleaned_1969_binarymask_500cluster.png']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d422d4-c615-46da-8e0d-f59aefea3696",
   "metadata": {},
   "source": [
    "Tile the image with patchify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00fc3a34-32c5-4e8a-8b09-19b75056e922",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_image_and_mask(image_path, mask_path, patch_size=256):\n",
    "    # Load the image and mask\n",
    "    image = np.array(Image.open(image_path).convert('RGB'))\n",
    "    mask = np.array(Image.open(mask_path))\n",
    "\n",
    "    # Check mask statistics before normalization\n",
    "    print(\"Mask stats before normalization: min={}, max={}\".format(mask.min(), mask.max()))\n",
    "\n",
    "    # Normalize the mask to have values between 0 and 1\n",
    "    mask = mask / 255.0\n",
    "\n",
    "    # Check mask statistics after normalization\n",
    "    print(\"Mask stats after normalization: min={}, max={}\".format(mask.min(), mask.max()))\n",
    "\n",
    "    # Ensure masks are 2D for simplicity\n",
    "    if len(mask.shape) > 2 and mask.shape[-1] == 1:\n",
    "        mask = mask[:, :, 0]\n",
    "\n",
    "    # Split the image and mask into patches\n",
    "    image_patches = patchify(image, (patch_size, patch_size, 3), step=patch_size)\n",
    "    mask_patches = patchify(mask, (patch_size, patch_size), step=patch_size)\n",
    "    \n",
    "    print(f\"Image patches shape: {image_patches.shape}\")\n",
    "    print(f\"Mask patches shape: {mask_patches.shape}\")\n",
    "\n",
    "    # Check patch statistics\n",
    "    print(\"First image patch stats: min={}, max={}\".format(image_patches[0, 0, 0, :, :, :].min(), image_patches[0, 0, 0, :, :, :].max()))\n",
    "    print(\"First mask patch stats: min={}, max={}\".format(mask_patches[0, 0, :, :].min(), mask_patches[0, 0, :, :].max()))\n",
    "    \n",
    "    return image_patches, mask_patches\n",
    "\n",
    "def filter_patches(image_patches, mask_patches):\n",
    "    all_img_patches = []\n",
    "    all_mask_patches = []\n",
    "\n",
    "    num_patches_x, num_patches_y = image_patches.shape[0], image_patches.shape[1]\n",
    "    for i in range(num_patches_x):\n",
    "        for j in range(num_patches_y):\n",
    "            # Remove the extra dimension\n",
    "            single_patch_img = image_patches[i, j, 0, :, :, :]  # (patch_size, patch_size, 3)\n",
    "            single_patch_mask = mask_patches[i, j, :, :]         # (patch_size, patch_size)\n",
    "\n",
    "            # Append all patches, including those that are entirely 0 or 1\n",
    "            all_img_patches.append(single_patch_img)\n",
    "            all_mask_patches.append(single_patch_mask)\n",
    "    \n",
    "    return np.array(all_img_patches), np.array(all_mask_patches)\n",
    "\n",
    "def process_and_save_images(image_paths, mask_paths, patch_size=256):\n",
    "    all_images = []\n",
    "    all_masks = []\n",
    "\n",
    "    for img_path, mask_path in zip(image_paths, mask_paths):\n",
    "        image_patches, mask_patches = split_image_and_mask(img_path, mask_path, patch_size)\n",
    "        \n",
    "        # Process patches without filtering\n",
    "        images, masks = filter_patches(image_patches, mask_patches)\n",
    "        all_images.append(images)\n",
    "        all_masks.append(masks)\n",
    "\n",
    "    # Concatenate all image and mask patches\n",
    "    all_images = np.concatenate(all_images)\n",
    "    all_masks = np.concatenate(all_masks)\n",
    "\n",
    "    return all_images, all_masks\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "def create_dataset(images, masks):\n",
    "    # Ensure images and masks are NumPy arrays\n",
    "    if not isinstance(images, np.ndarray) or not isinstance(masks, np.ndarray):\n",
    "        raise ValueError(\"Images and masks must be NumPy arrays.\")\n",
    "\n",
    "    # Convert the NumPy arrays to Pillow images and store them in a dictionary\n",
    "    dataset_dict = {\n",
    "        \"image\": [Image.fromarray(img) for img in images],\n",
    "        \"label\": [Image.fromarray(mask) for mask in masks],\n",
    "    }\n",
    "\n",
    "    # Create the dataset using the datasets.Dataset class\n",
    "    dataset = Dataset.from_dict(dataset_dict)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8ac07ee-7362-4e53-b861-fab343518f30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sking11/.conda/envs/dino_env/lib/python3.12/site-packages/PIL/Image.py:3368: DecompressionBombWarning: Image size (122608150 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mask stats before normalization: min=0, max=255\n",
      "Mask stats after normalization: min=0.0, max=1.0\n",
      "Image patches shape: (37, 49, 1, 256, 256, 3)\n",
      "Mask patches shape: (37, 49, 256, 256)\n",
      "First image patch stats: min=0, max=255\n",
      "First mask patch stats: min=0.0, max=0.0\n"
     ]
    }
   ],
   "source": [
    "val_images, val_masks = process_and_save_images(image_paths_val, label_paths_val)\n",
    "val_dataset = create_dataset(val_images, val_masks)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=4, shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f60b3a1-24c6-41ab-a153-625201e153dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1813\n"
     ]
    }
   ],
   "source": [
    "print(len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "624afde1-7585-48a8-b57a-5b4e2e5360de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bounding_box(ground_truth_map):\n",
    "    y_indices, x_indices = np.where(ground_truth_map > 0)\n",
    "    \n",
    "    if len(x_indices) == 0 or len(y_indices) == 0:\n",
    "        # Handle empty mask case\n",
    "        # You might return a default bounding box or skip processing\n",
    "        return [0, 0, 1, 1]  # Example default bounding box, adjust if needed\n",
    "\n",
    "    x_min, x_max = np.min(x_indices), np.max(x_indices)\n",
    "    y_min, y_max = np.min(y_indices), np.max(y_indices)\n",
    "    \n",
    "    # Add some margin to the bounding box if needed\n",
    "    x_min = max(0, x_min - 10)\n",
    "    x_max = min(ground_truth_map.shape[1], x_max + 10)\n",
    "    y_min = max(0, y_min - 10)\n",
    "    y_max = min(ground_truth_map.shape[0], y_max + 10)\n",
    "    \n",
    "    return [x_min, y_min, x_max, y_max]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63b1942f-b1b3-4653-8c04-4dd84d68a1e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2426565/1093081830.py:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  my_mito_model.load_state_dict(torch.load('/explore/nobackup/people/sking11/sam_model_checkpoint_6400.pth'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#set device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load the model configuration\n",
    "model_config = SamConfig.from_pretrained(\"facebook/sam-vit-base\")\n",
    "processor = SamProcessor.from_pretrained(\"facebook/sam-vit-base\")\n",
    "\n",
    "# Create an instance of the model architecture with the loaded configuration\n",
    "my_mito_model = SamModel(config=model_config)\n",
    "#Update the model by loading the weights from saved file.\n",
    "my_mito_model.load_state_dict(torch.load('/explore/nobackup/people/sking11/sam_model_checkpoint_6400.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a85ed75-be06-443e-b284-99fd2e244e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to generate predicted masks\n",
    "def generate_predicted_masks(val_dataset, model, processor, device):\n",
    "    predicted_masks = []\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    for idx in range(len(val_dataset)):\n",
    "        # Load image\n",
    "        test_image = val_dataset[idx][\"image\"]\n",
    "\n",
    "        # Get box prompt based on ground truth segmentation map\n",
    "        ground_truth_mask = np.array(val_dataset[idx][\"label\"])\n",
    "        prompt = get_bounding_box(ground_truth_mask)  # Ensure the function is consistent\n",
    "\n",
    "        # Prepare image + box prompt for the model\n",
    "        inputs = processor(test_image, input_boxes=[[prompt]], return_tensors=\"pt\")\n",
    "\n",
    "        # Move the input tensor to the GPU if it's not already there\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "        # Forward pass\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs, multimask_output=False)\n",
    "\n",
    "        # Apply sigmoid to get the probability map\n",
    "        medsam_seg_prob = torch.sigmoid(outputs.pred_masks.squeeze(1))\n",
    "\n",
    "        # Convert soft mask to hard mask and move to CPU\n",
    "        medsam_seg_prob = medsam_seg_prob.cpu().numpy().squeeze()\n",
    "        medsam_seg = (medsam_seg_prob > 0.5).astype(np.uint8)\n",
    "\n",
    "        # Save the predicted mask as PNG\n",
    "        mask_image = Image.fromarray(medsam_seg * 255)  # Convert binary mask to [0, 255] range\n",
    "        mask_image.save(os.path.join(output_dir, f\"predicted_mask_{idx}.png\"))\n",
    "\n",
    "        # Append the predicted mask to the list\n",
    "        predicted_masks.append(medsam_seg)\n",
    "\n",
    "    return predicted_masks\n",
    "\n",
    "# Example usage\n",
    "output_dir = \"/explore/nobackup/people/sking11/SAM_output_masks1\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "val_predicted_masks = generate_predicted_masks(val_dataset, my_mito_model, processor, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a54c8a1c-e518-4af1-809e-8179ced43fbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1813\n"
     ]
    }
   ],
   "source": [
    "print(len(val_predicted_masks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "495804eb-d20e-4edb-9d48-630cb9e6df74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "def unpatchify_binary(patches_list, patch_size, num_patches_x, num_patches_y):\n",
    "    \"\"\"\n",
    "    Reconstruct the full binary mask from a list of patches.\n",
    "    \n",
    "    Args:\n",
    "        patches_list (list of numpy.ndarray): List of patch arrays with binary values.\n",
    "        patch_size (tuple): Size of each patch (height, width).\n",
    "        num_patches_x (int): Number of patches along the height.\n",
    "        num_patches_y (int): Number of patches along the width.\n",
    "        \n",
    "    Returns:\n",
    "        numpy.ndarray: The reconstructed full-size binary mask.\n",
    "    \"\"\"\n",
    "    patch_height, patch_width = patch_size\n",
    "    full_height = num_patches_x * patch_height\n",
    "    full_width = num_patches_y * patch_width\n",
    "    \n",
    "    # Initialize the full-size mask array\n",
    "    full_mask = np.zeros((full_height, full_width), dtype=np.float32)\n",
    "    \n",
    "    # Initialize a count array to handle overlapping patches (if any)\n",
    "    count_array = np.zeros((full_height, full_width), dtype=np.float32)\n",
    "    \n",
    "    # Place each patch in the correct position\n",
    "    index = 0\n",
    "    for i in range(num_patches_x):\n",
    "        for j in range(num_patches_y):\n",
    "            if index >= len(patches_list):\n",
    "                print(f\"Warning: Expected more patches but reached the end of the list.\")\n",
    "                break\n",
    "            \n",
    "            patch = patches_list[index]\n",
    "            start_x = i * patch_height\n",
    "            start_y = j * patch_width\n",
    "            \n",
    "            full_mask[start_x:start_x+patch_height, start_y:start_y+patch_width] += patch\n",
    "            count_array[start_x:start_x+patch_height, start_y:start_y+patch_width] += 1\n",
    "            \n",
    "            index += 1\n",
    "    \n",
    "    # Normalize if any overlapping patches were averaged\n",
    "    full_mask = np.divide(full_mask, count_array, out=np.zeros_like(full_mask, dtype=np.float32), where=count_array!=0).astype(np.uint8)\n",
    "    \n",
    "    # Threshold to ensure binary output (0 or 1)\n",
    "    full_mask = np.clip(full_mask, 0, 1)\n",
    "\n",
    "    return full_mask\n",
    "\n",
    "# Example usage:\n",
    "num_patches_x = 37  # Number of rows\n",
    "num_patches_y = 49  # Number of columns\n",
    "patch_size = (256, 256)  # Height, width of each patch\n",
    "\n",
    "# Reconstruct the full binary mask from the list\n",
    "full_mask = unpatchify_binary(val_predicted_masks, patch_size, num_patches_x, num_patches_y)\n",
    "\n",
    "# Save or visualize the results\n",
    "# Convert binary mask to 'L' mode for saving\n",
    "Image.fromarray(full_mask * 255).convert('L').save('/explore/nobackup/people/sking11/fullmaskSAM11212.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa706681-9472-4bc4-8d4e-592740ff23cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "def unpatchify_rgb(patches_list, patch_size, num_patches_x, num_patches_y):\n",
    "    \"\"\"\n",
    "    Reconstruct the full RGB image from a list of patches.\n",
    "    \n",
    "    Args:\n",
    "        patches_list (list of numpy.ndarray): List of patch arrays with RGB values.\n",
    "        patch_size (tuple): Size of each patch (height, width).\n",
    "        num_patches_x (int): Number of patches along the height.\n",
    "        num_patches_y (int): Number of patches along the width.\n",
    "        \n",
    "    Returns:\n",
    "        numpy.ndarray: The reconstructed full-size RGB image.\n",
    "    \"\"\"\n",
    "    patch_height, patch_width = patch_size\n",
    "    full_height = num_patches_x * patch_height\n",
    "    full_width = num_patches_y * patch_width\n",
    "    \n",
    "    # Initialize the full-size image array\n",
    "    full_image = np.zeros((full_height, full_width, 3), dtype=np.float32)\n",
    "    \n",
    "    # Initialize a count array to handle overlapping patches (if any)\n",
    "    count_array = np.zeros((full_height, full_width, 3), dtype=np.float32)\n",
    "    \n",
    "    # Place each patch in the correct position\n",
    "    index = 0\n",
    "    for i in range(num_patches_x):\n",
    "        for j in range(num_patches_y):\n",
    "            if index >= len(patches_list):\n",
    "                print(f\"Warning: Expected more patches but reached the end of the list.\")\n",
    "                break\n",
    "            \n",
    "            patch = patches_list[index]\n",
    "            start_x = i * patch_height\n",
    "            start_y = j * patch_width\n",
    "            \n",
    "            full_image[start_x:start_x+patch_height, start_y:start_y+patch_width, :] += patch\n",
    "            count_array[start_x:start_x+patch_height, start_y:start_y+patch_width, :] += 1\n",
    "            \n",
    "            index += 1\n",
    "    \n",
    "    # Normalize if any overlapping patches were averaged\n",
    "    full_image = np.divide(full_image, count_array, out=np.zeros_like(full_image, dtype=np.float32), where=count_array!=0).astype(np.uint8)\n",
    "    \n",
    "    return full_image\n",
    "\n",
    "# Example usage:\n",
    "num_patches_x = 37  # Number of rows\n",
    "num_patches_y = 49  # Number of columns\n",
    "patch_size = (256, 256)  # Height, width of each patch\n",
    "\n",
    "# Reconstruct the full RGB image from the list\n",
    "full_image = unpatchify_rgb(val_images, patch_size, num_patches_x, num_patches_y)\n",
    "\n",
    "# Save or visualize the results\n",
    "# Convert RGB image for saving\n",
    "Image.fromarray(full_image).save('/explore/nobackup/people/sking11/full_imageSAM_val1212.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dino_env",
   "language": "python",
   "name": "dino_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
