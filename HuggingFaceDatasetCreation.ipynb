{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2aaba943-27e6-4033-8d27-ad86e9413467",
   "metadata": {},
   "source": [
    "# Code for creating a huggingface dataset out of high-rez satellite images. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec51864-7f6f-40e1-a17d-8a2c9f169fad",
   "metadata": {},
   "source": [
    "Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "149641ad-03a6-440c-8346-774920cf839e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image as PILImage\n",
    "import os\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from patchify import patchify\n",
    "from datasets import load_dataset, DatasetDict, Dataset, Image as HFImage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a321320-35ae-4748-9841-6f682952b3a3",
   "metadata": {},
   "source": [
    "Login to hugging face if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14537768-6870-4c4c-a35a-9109b2b95c03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "Your token has been saved to /home/sking11/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "import huggingface_hub\n",
    "\n",
    "huggingface_hub.login(token = 'insert your token here')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4efd41ec-cf03-4137-ac29-9a8bc21e5624",
   "metadata": {},
   "source": [
    "Create your lists of training images, validation images, and test images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "122f12ea-66c7-4e31-9cd2-4e15574b6749",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training dataset\n",
    "image_paths_train = ['/explore/nobackup/people/sking11/MakingDinoDataset/gliht_1964_rgbi.tif', \n",
    "                     '/explore/nobackup/people/sking11/MakingDinoDataset/gliht_1965_rgbi.tif',\n",
    "                     '/explore/nobackup/people/sking11/MakingDinoDataset/gliht_1966_rgbi.tif',\n",
    "                     '/explore/nobackup/people/sking11/MakingDinoDataset/gliht_1967_rgbi.tif',\n",
    "                     '/explore/nobackup/people/sking11/MakingDinoDataset/gliht_1968_rgbi.tif']\n",
    "\n",
    "label_paths_train = ['/explore/nobackup/people/sking11/MakingDinoDataset/binarymasks/cleaned_1964_binarymask_600cluster.png', \n",
    "                     '/explore/nobackup/people/sking11/MakingDinoDataset/binarymasks/cleaned_1965_binarymask_600cluster.png',\n",
    "                     '/explore/nobackup/people/sking11/MakingDinoDataset/binarymasks/cleaned_1966_binarymask_600cluster.png',\n",
    "                     '/explore/nobackup/people/sking11/MakingDinoDataset/binarymasks/cleaned_1967_binarymask_400cluster.png',\n",
    "                     '/explore/nobackup/people/sking11/MakingDinoDataset/binarymasks/cleaned_1968_binarymask_300cluster.png']\n",
    "\n",
    "#cross validation dataset\n",
    "image_paths_val = ['/explore/nobackup/people/sking11/MakingDinoDataset/gliht_1969_rgbi.tif']\n",
    "label_paths_val = ['/explore/nobackup/people/sking11/MakingDinoDataset/binarymasks/cleaned_1969_binarymask_500cluster.png']\n",
    "\n",
    "#test dataset\n",
    "image_paths_test = ['/explore/nobackup/people/sking11/MakingDinoDataset/gliht_1970_rgbi.tif']\n",
    "label_paths_test = ['/explore/nobackup/people/sking11/MakingDinoDataset/binarymasks/cleaned_1970_binarymask_600cluster.png']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085610cb-b1fe-4638-b804-071e02043e8c",
   "metadata": {},
   "source": [
    "This set of functions tiles the images and masks into your specified patch size, removes the any pairs of images and masks where the masks are \"empty\" (only contain 0 or 1 values), and saves the images and masks as lists of numpy arrays. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ad10b9d-cb25-40e7-862a-471edb0468b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sking11/.conda/envs/dino_env/lib/python3.12/site-packages/PIL/Image.py:3368: DecompressionBombWarning: Image size (122792715 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image patches shape: (38, 48, 1, 256, 256, 3)\n",
      "Mask patches shape: (38, 48, 256, 256)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sking11/.conda/envs/dino_env/lib/python3.12/site-packages/PIL/Image.py:3368: DecompressionBombWarning: Image size (127073662 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image patches shape: (39, 49, 1, 256, 256, 3)\n",
      "Mask patches shape: (39, 49, 256, 256)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sking11/.conda/envs/dino_env/lib/python3.12/site-packages/PIL/Image.py:3368: DecompressionBombWarning: Image size (117097890 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image patches shape: (36, 48, 1, 256, 256, 3)\n",
      "Mask patches shape: (36, 48, 256, 256)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sking11/.conda/envs/dino_env/lib/python3.12/site-packages/PIL/Image.py:3368: DecompressionBombWarning: Image size (123492096 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image patches shape: (37, 50, 1, 256, 256, 3)\n",
      "Mask patches shape: (37, 50, 256, 256)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sking11/.conda/envs/dino_env/lib/python3.12/site-packages/PIL/Image.py:3368: DecompressionBombWarning: Image size (124371150 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image patches shape: (37, 50, 1, 256, 256, 3)\n",
      "Mask patches shape: (37, 50, 256, 256)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sking11/.conda/envs/dino_env/lib/python3.12/site-packages/PIL/Image.py:3368: DecompressionBombWarning: Image size (122608150 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image patches shape: (37, 49, 1, 256, 256, 3)\n",
      "Mask patches shape: (37, 49, 256, 256)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sking11/.conda/envs/dino_env/lib/python3.12/site-packages/PIL/Image.py:3368: DecompressionBombWarning: Image size (122890173 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image patches shape: (38, 48, 1, 256, 256, 3)\n",
      "Mask patches shape: (38, 48, 256, 256)\n"
     ]
    }
   ],
   "source": [
    "def split_image_and_mask(image_path, mask_path, patch_size=256):\n",
    "    # Load the image and mask\n",
    "    image = np.array(PILImage.open(image_path).convert('RGB'))\n",
    "    mask = np.array(PILImage.open(mask_path))\n",
    "\n",
    "    # Ensure masks are 2D for simplicity\n",
    "    if len(mask.shape) > 2 and mask.shape[-1] == 1:\n",
    "        mask = mask[:, :, 0]\n",
    "\n",
    "    # Split the image and mask into patches\n",
    "    image_patches = patchify(image, (patch_size, patch_size, 3), step=patch_size)\n",
    "    mask_patches = patchify(mask, (patch_size, patch_size), step=patch_size)\n",
    "    \n",
    "    print(f\"Image patches shape: {image_patches.shape}\")\n",
    "    print(f\"Mask patches shape: {mask_patches.shape}\")\n",
    "    \n",
    "    return image_patches, mask_patches\n",
    "\n",
    "def filter_patches(image_patches, mask_patches):\n",
    "    all_img_patches = []\n",
    "    all_mask_patches = []\n",
    "\n",
    "    num_patches_x, num_patches_y = image_patches.shape[0], image_patches.shape[1]\n",
    "    for i in range(num_patches_x):\n",
    "        for j in range(num_patches_y):\n",
    "            # Remove the extra dimension\n",
    "            single_patch_img = image_patches[i, j, 0, :, :, :]  # (patch_size, patch_size, 3)\n",
    "            single_patch_mask = mask_patches[i, j, :, :]         # (patch_size, patch_size)\n",
    "\n",
    "            # Check if the mask patch contains only one unique value\n",
    "            unique_values = np.unique(single_patch_mask)\n",
    "            if len(unique_values) > 1:\n",
    "                all_img_patches.append(single_patch_img)\n",
    "                all_mask_patches.append(single_patch_mask)\n",
    "    \n",
    "    return np.array(all_img_patches), np.array(all_mask_patches)\n",
    "\n",
    "def process_and_save_images(image_paths, mask_paths, patch_size=256):\n",
    "    all_images = []\n",
    "    all_masks = []\n",
    "\n",
    "    for img_path, mask_path in zip(image_paths, mask_paths):\n",
    "        image_patches, mask_patches = split_image_and_mask(img_path, mask_path, patch_size)\n",
    "        \n",
    "        # Filter out empty patches\n",
    "        images, masks = filter_patches(image_patches, mask_patches)\n",
    "        all_images.append(images)\n",
    "        all_masks.append(masks)\n",
    "\n",
    "    # Concatenate all valid image and mask patches\n",
    "    all_images = np.concatenate(all_images)\n",
    "    all_masks = np.concatenate(all_masks)\n",
    "\n",
    "    # Filter out empty masks\n",
    "    valid_indices = [i for i, mask in enumerate(all_masks) if mask.max() != 0]\n",
    "    filtered_images = all_images[valid_indices]\n",
    "    filtered_masks = all_masks[valid_indices]\n",
    "\n",
    "    return filtered_images, filtered_masks\n",
    "\n",
    "train_images, train_masks = process_and_save_images(image_paths_train, label_paths_train)\n",
    "val_images, val_masks = process_and_save_images(image_paths_val, label_paths_val)\n",
    "test_images, test_masks = process_and_save_images(image_paths_test, label_paths_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f09c022-5a72-4ca6-98aa-b788d2597b70",
   "metadata": {},
   "source": [
    "Function to turn the lists of image and mask arrays into a training, validation, and test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4b73911-1811-44ae-9b6d-b7113944958e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(image_arrays, mask_arrays):\n",
    "    image_list = []\n",
    "    mask_list = []\n",
    "\n",
    "    for img_array, mask_array in zip(image_arrays, mask_arrays):\n",
    "        # Convert NumPy arrays to PIL images\n",
    "        img = PILImage.fromarray(img_array)\n",
    "        mask = PILImage.fromarray(mask_array)\n",
    "\n",
    "        # Convert PIL images to a format that can be used in the Hugging Face dataset\n",
    "        img = np.array(img)  # Convert back to NumPy if needed\n",
    "        mask = np.array(mask)\n",
    "\n",
    "        image_list.append(img)\n",
    "        mask_list.append(mask)\n",
    "\n",
    "    dataset = Dataset.from_dict({\"image\": image_list, \"label\": mask_list})\n",
    "    dataset = dataset.cast_column(\"image\", HFImage())\n",
    "    dataset = dataset.cast_column(\"label\", HFImage())\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e851331b-ba1f-4d09-b7b9-7410932b46ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sking11/.conda/envs/dino_env/lib/python3.12/site-packages/datasets/features/image.py:339: UserWarning: Downcasting array dtype int64 to uint8 to be compatible with 'Pillow'\n",
      "  warnings.warn(f\"Downcasting array dtype {dtype} to {dest_dtype} to be compatible with 'Pillow'\")\n",
      "/home/sking11/.conda/envs/dino_env/lib/python3.12/site-packages/datasets/features/image.py:348: UserWarning: Downcasting array dtype int64 to int32 to be compatible with 'Pillow'\n",
      "  warnings.warn(f\"Downcasting array dtype {dtype} to {dest_dtype} to be compatible with 'Pillow'\")\n"
     ]
    }
   ],
   "source": [
    "train_dataset = create_dataset(train_images, binary_train_masks)\n",
    "val_dataset = create_dataset(val_images, binary_val_masks)\n",
    "test_dataset = create_dataset(test_images, binary_test_masks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ae5979-ef9d-478e-9699-e193683ba49b",
   "metadata": {},
   "source": [
    "Compile your train, validation, and test dataset into a single huggingface formatted dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc019d80-eb4f-48eb-8ce1-1c39a9712dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"validation\": val_dataset,\n",
    "    \"test\": test_dataset,\n",
    "  }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d2dab0-eb4f-46ae-863f-ff4ae7474079",
   "metadata": {},
   "source": [
    "Upload your dataset to huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "87a25bf7-6c5a-4e05-a896-252571326b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading the dataset shards:   0%|          | 0/6 [00:00<?, ?it/s]\n",
      "Map:   0%|          | 0/1067 [00:00<?, ? examples/s]\u001b[A\n",
      "Map: 100%|██████████| 1067/1067 [00:00<00:00, 1351.48 examples/s]\u001b[A\n",
      "\n",
      "Creating parquet from Arrow format:   0%|          | 0/11 [00:00<?, ?ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  18%|█▊        | 2/11 [00:00<00:00, 12.36ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  36%|███▋      | 4/11 [00:00<00:00, 11.26ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  55%|█████▍    | 6/11 [00:00<00:00, 11.06ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  73%|███████▎  | 8/11 [00:00<00:00, 11.97ba/s]\u001b[A\n",
      "Creating parquet from Arrow format: 100%|██████████| 11/11 [00:00<00:00, 12.22ba/s]\u001b[A\n",
      "Uploading the dataset shards:  17%|█▋        | 1/6 [00:11<00:55, 11.14s/it]\n",
      "Map:   0%|          | 0/1067 [00:00<?, ? examples/s]\u001b[A\n",
      "Map: 100%|██████████| 1067/1067 [00:00<00:00, 1459.31 examples/s]\u001b[A\n",
      "\n",
      "Creating parquet from Arrow format:   0%|          | 0/11 [00:00<?, ?ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  18%|█▊        | 2/11 [00:00<00:00, 13.42ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  36%|███▋      | 4/11 [00:00<00:00, 12.23ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  55%|█████▍    | 6/11 [00:00<00:00, 11.78ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  73%|███████▎  | 8/11 [00:00<00:00, 12.06ba/s]\u001b[A\n",
      "Creating parquet from Arrow format: 100%|██████████| 11/11 [00:00<00:00, 12.53ba/s]\u001b[A\n",
      "Uploading the dataset shards:  33%|███▎      | 2/6 [00:19<00:37,  9.42s/it]\n",
      "Map:   0%|          | 0/1067 [00:00<?, ? examples/s]\u001b[A\n",
      "Map: 100%|██████████| 1067/1067 [00:00<00:00, 1487.51 examples/s]\u001b[A\n",
      "\n",
      "Creating parquet from Arrow format:   0%|          | 0/11 [00:00<?, ?ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  18%|█▊        | 2/11 [00:00<00:00, 13.70ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  36%|███▋      | 4/11 [00:00<00:00, 12.50ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  55%|█████▍    | 6/11 [00:00<00:00, 12.46ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  73%|███████▎  | 8/11 [00:00<00:00, 12.36ba/s]\u001b[A\n",
      "Creating parquet from Arrow format: 100%|██████████| 11/11 [00:00<00:00, 12.71ba/s]\u001b[A\n",
      "Uploading the dataset shards:  50%|█████     | 3/6 [00:28<00:28,  9.39s/it]\n",
      "Map:   0%|          | 0/1067 [00:00<?, ? examples/s]\u001b[A\n",
      "Map: 100%|██████████| 1067/1067 [00:00<00:00, 1413.67 examples/s]\u001b[A\n",
      "\n",
      "Creating parquet from Arrow format:   0%|          | 0/11 [00:00<?, ?ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  18%|█▊        | 2/11 [00:00<00:00, 12.39ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  36%|███▋      | 4/11 [00:00<00:00, 11.75ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  55%|█████▍    | 6/11 [00:00<00:00, 11.55ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  73%|███████▎  | 8/11 [00:00<00:00, 12.44ba/s]\u001b[A\n",
      "Creating parquet from Arrow format: 100%|██████████| 11/11 [00:00<00:00, 12.59ba/s]\u001b[A\n",
      "Uploading the dataset shards:  67%|██████▋   | 4/6 [00:39<00:19,  9.87s/it]\n",
      "Map:   0%|          | 0/1066 [00:00<?, ? examples/s]\u001b[A\n",
      "Map: 100%|██████████| 1066/1066 [00:00<00:00, 1428.94 examples/s]\u001b[A\n",
      "\n",
      "Creating parquet from Arrow format:   0%|          | 0/11 [00:00<?, ?ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  18%|█▊        | 2/11 [00:00<00:00, 13.47ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  36%|███▋      | 4/11 [00:00<00:00, 12.22ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  55%|█████▍    | 6/11 [00:00<00:00, 12.17ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  73%|███████▎  | 8/11 [00:00<00:00, 13.00ba/s]\u001b[A\n",
      "Creating parquet from Arrow format: 100%|██████████| 11/11 [00:00<00:00, 13.23ba/s]\u001b[A\n",
      "Uploading the dataset shards:  83%|████████▎ | 5/6 [00:47<00:09,  9.31s/it]\n",
      "Map:   0%|          | 0/1066 [00:00<?, ? examples/s]\u001b[A\n",
      "Map: 100%|██████████| 1066/1066 [00:00<00:00, 1319.56 examples/s]\u001b[A\n",
      "\n",
      "Creating parquet from Arrow format:   0%|          | 0/11 [00:00<?, ?ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  18%|█▊        | 2/11 [00:00<00:00, 13.37ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  36%|███▋      | 4/11 [00:00<00:00, 12.21ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  55%|█████▍    | 6/11 [00:00<00:00, 12.02ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  73%|███████▎  | 8/11 [00:00<00:00, 12.94ba/s]\u001b[A\n",
      "Creating parquet from Arrow format: 100%|██████████| 11/11 [00:00<00:00, 13.14ba/s]\u001b[A\n",
      "Uploading the dataset shards: 100%|██████████| 6/6 [00:54<00:00,  9.14s/it]\n",
      "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Map:   0%|          | 0/653 [00:00<?, ? examples/s]\u001b[A\n",
      "Map: 100%|██████████| 653/653 [00:00<00:00, 1214.57 examples/s]\u001b[A\n",
      "\n",
      "Creating parquet from Arrow format:   0%|          | 0/7 [00:00<?, ?ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  29%|██▊       | 2/7 [00:00<00:00, 13.84ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  57%|█████▋    | 4/7 [00:00<00:00, 12.42ba/s]\u001b[A\n",
      "Creating parquet from Arrow format: 100%|██████████| 7/7 [00:00<00:00, 13.44ba/s]\u001b[A\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:05<00:00,  5.44s/it]\n",
      "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Map:   0%|          | 0/1286 [00:00<?, ? examples/s]\u001b[A\n",
      "Map:  78%|███████▊  | 1000/1286 [00:00<00:00, 1531.03 examples/s]\u001b[A\n",
      "Map: 100%|██████████| 1286/1286 [00:00<00:00, 1466.21 examples/s]\u001b[A\n",
      "\n",
      "Creating parquet from Arrow format:   0%|          | 0/13 [00:00<?, ?ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  15%|█▌        | 2/13 [00:00<00:00, 13.69ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  31%|███       | 4/13 [00:00<00:00, 12.40ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  46%|████▌     | 6/13 [00:00<00:00, 12.17ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  62%|██████▏   | 8/13 [00:00<00:00, 12.45ba/s]\u001b[A\n",
      "Creating parquet from Arrow format:  77%|███████▋  | 10/13 [00:00<00:00, 12.39ba/s]\u001b[A\n",
      "Creating parquet from Arrow format: 100%|██████████| 13/13 [00:00<00:00, 13.03ba/s]\u001b[A\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:10<00:00, 10.38s/it]\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/saking3/alaska_dead_trees/commit/1d42386b17ab1e911c537f48ad7960cc16179734', commit_message='Upload dataset', commit_description='', oid='1d42386b17ab1e911c537f48ad7960cc16179734', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.push_to_hub('yourusername/yourdataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4290170-552f-47c2-b7a6-6171ce006245",
   "metadata": {},
   "source": [
    "test if your dataset is downloadable and view what info is contained in your dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "16669442-7139-4a82-8ee9-17e7500ce6d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 1.40k/1.40k [00:00<00:00, 13.3MB/s]\n",
      "Downloading data: 100%|██████████| 156M/156M [00:03<00:00, 40.5MB/s] \n",
      "Downloading data: 100%|██████████| 152M/152M [00:04<00:00, 36.6MB/s] \n",
      "Downloading data: 100%|██████████| 158M/158M [00:03<00:00, 41.7MB/s] \n",
      "Downloading data: 100%|██████████| 161M/161M [00:04<00:00, 37.7MB/s] \n",
      "Downloading data: 100%|██████████| 154M/154M [00:03<00:00, 43.0MB/s] \n",
      "Downloading data: 100%|██████████| 153M/153M [00:03<00:00, 43.5MB/s] \n",
      "Downloading data: 100%|██████████| 90.3M/90.3M [00:02<00:00, 35.0MB/s]\n",
      "Downloading data: 100%|██████████| 177M/177M [00:04<00:00, 36.4MB/s] \n",
      "Generating train split: 100%|██████████| 6400/6400 [00:05<00:00, 1188.70 examples/s]\n",
      "Generating validation split: 100%|██████████| 653/653 [00:00<00:00, 1252.38 examples/s]\n",
      "Generating test split: 100%|██████████| 1286/1286 [00:01<00:00, 1229.63 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset('saking3/alaska_dead_trees')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "085b0682-4466-45c9-bd3a-3cb996d7be1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['image', 'label'],\n",
       "        num_rows: 6400\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['image', 'label'],\n",
       "        num_rows: 653\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['image', 'label'],\n",
       "        num_rows: 1286\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dino_env",
   "language": "python",
   "name": "dino_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
