{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c4d6fff-6a99-441f-a278-a2bc12a7aa00",
   "metadata": {},
   "source": [
    "This notebook is for running predictions on an entire image (rather than just non-empty tiles) and stitching them together into a single image of predictions using the DINOv2 Model. You will need a saved, trained DINOv2 model to load in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2a8dfdcb-66ab-44a6-88b8-9f40f180730a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#packages\n",
    "\n",
    "from datasets import load_dataset\n",
    "from PIL import Image\n",
    "import os\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import albumentations as A\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from patchify import patchify\n",
    "from transformers import Dinov2Model, Dinov2PreTrainedModel\n",
    "from transformers.modeling_outputs import SemanticSegmenterOutput\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec481d3-7eee-4f98-a04c-d2a8509ef069",
   "metadata": {},
   "source": [
    "# Loading and Preparing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da20f24f-10f5-4840-9757-b4fa32e1db9c",
   "metadata": {},
   "source": [
    "Load in the image you would like to perform predictions on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "11aa9835-eaa3-410d-be86-a8ebd1ee2596",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset\n",
    "image_paths_val = ['/explore/nobackup/people/sking11/MakingDinoDataset/gliht_1969_rgbi.tif']\n",
    "label_paths_val = ['/explore/nobackup/people/sking11/MakingDinoDataset/binarymasks/cleaned_1969_binarymask_500cluster.png']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0504c1f-8827-4726-a1eb-01aa55f247cf",
   "metadata": {},
   "source": [
    "Tile the image with patchify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "271270b0-e99b-48f1-8932-b383837b9ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_image_and_mask(image_path, mask_path, patch_size=256):\n",
    "    # Load the image and mask\n",
    "    image = np.array(Image.open(image_path).convert('RGB'))\n",
    "    mask = np.array(Image.open(mask_path))\n",
    "\n",
    "    # Ensure masks are 2D for simplicity\n",
    "    if len(mask.shape) > 2 and mask.shape[-1] == 1:\n",
    "        mask = mask[:, :, 0]\n",
    "\n",
    "    # Split the image and mask into patches\n",
    "    image_patches = patchify(image, (patch_size, patch_size, 3), step=patch_size)\n",
    "    mask_patches = patchify(mask, (patch_size, patch_size), step=patch_size)\n",
    "    \n",
    "    print(f\"Image patches shape: {image_patches.shape}\")\n",
    "    print(f\"Mask patches shape: {mask_patches.shape}\")\n",
    "    \n",
    "    return image_patches, mask_patches\n",
    "\n",
    "def filter_patches(image_patches, mask_patches):\n",
    "    all_img_patches = []\n",
    "    all_mask_patches = []\n",
    "\n",
    "    num_patches_x, num_patches_y = image_patches.shape[0], image_patches.shape[1]\n",
    "    for i in range(num_patches_x):\n",
    "        for j in range(num_patches_y):\n",
    "            # Remove the extra dimension\n",
    "            single_patch_img = image_patches[i, j, 0, :, :, :]  # (patch_size, patch_size, 3)\n",
    "            single_patch_mask = mask_patches[i, j, :, :]         # (patch_size, patch_size)\n",
    "\n",
    "            # Directly append patches without filtering\n",
    "            all_img_patches.append(single_patch_img)\n",
    "            all_mask_patches.append(single_patch_mask)\n",
    "    \n",
    "    return np.array(all_img_patches), np.array(all_mask_patches)\n",
    "\n",
    "def process_and_save_images(image_paths, mask_paths, patch_size=256):\n",
    "    all_images = []\n",
    "    all_masks = []\n",
    "\n",
    "    for img_path, mask_path in zip(image_paths, mask_paths):\n",
    "        image_patches, mask_patches = split_image_and_mask(img_path, mask_path, patch_size)\n",
    "        \n",
    "        # Process patches without filtering\n",
    "        images, masks = filter_patches(image_patches, mask_patches)\n",
    "        all_images.append(images)\n",
    "        all_masks.append(masks)\n",
    "\n",
    "    # Concatenate all image and mask patches\n",
    "    all_images = np.concatenate(all_images)\n",
    "    all_masks = np.concatenate(all_masks)\n",
    "\n",
    "    return all_images, all_masks\n",
    "\n",
    "class CustomSegmentationDataset(Dataset):\n",
    "    def __init__(self, images, masks, transform=None):\n",
    "        self.images = images\n",
    "        self.masks = masks\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        original_image = self.images[idx]\n",
    "        original_segmentation_map = self.masks[idx]\n",
    "\n",
    "        # Ensure image and mask sizes match\n",
    "        assert original_image.shape[:2] == original_segmentation_map.shape[:2], \\\n",
    "            f\"Image and mask shape mismatch: {original_image.shape[:2]} vs {original_segmentation_map.shape[:2]}\"\n",
    "\n",
    "        # Convert masks to binary values: 0 and 1\n",
    "        original_segmentation_map = (original_segmentation_map / 255).astype(np.uint8)\n",
    "\n",
    "        if self.transform:\n",
    "            transformed = self.transform(image=original_image, mask=original_segmentation_map)\n",
    "            image = transformed['image']\n",
    "            target = transformed['mask']\n",
    "        else:\n",
    "            image = original_image\n",
    "            target = original_segmentation_map\n",
    "\n",
    "        # Convert to tensor\n",
    "        image = torch.tensor(image).float()\n",
    "        target = torch.tensor(target).long()\n",
    "\n",
    "        # Convert to C, H, W if necessary\n",
    "        if image.ndimension() == 3 and image.shape[-1] in {1, 3}:\n",
    "            image = image.permute(2, 0, 1)  # Convert to C, H, W\n",
    "\n",
    "        return image, target, original_image, original_segmentation_map\n",
    "\n",
    "ADE_MEAN = np.array([123.675, 116.280, 103.530]) / 255\n",
    "ADE_STD = np.array([58.395, 57.120, 57.375]) / 255\n",
    "\n",
    "val_transform = A.Compose([\n",
    "    A.Resize(width=448, height=448),\n",
    "    A.Normalize(mean=ADE_MEAN.tolist(), std=ADE_STD.tolist()),\n",
    "])\n",
    "\n",
    "def collate_fn(inputs):\n",
    "    batch = dict()\n",
    "    pixel_values = torch.stack([torch.tensor(i[0]).float() for i in inputs], dim=0)\n",
    "    labels = torch.stack([torch.tensor(i[1]).long() for i in inputs], dim=0)\n",
    "    original_images = [torch.tensor(i[2]).float() for i in inputs]\n",
    "    original_segmentation_maps = [torch.tensor(i[3]).long() for i in inputs]\n",
    "\n",
    "    # Uncomment this if your pixel_values are in (batch_size, height, width, channels) format\n",
    "    # pixel_values = pixel_values.permute(0, 3, 1, 2)\n",
    "\n",
    "    batch[\"pixel_values\"] = pixel_values\n",
    "    batch[\"labels\"] = labels\n",
    "    batch[\"original_images\"] = original_images\n",
    "    batch[\"original_segmentation_maps\"] = original_segmentation_maps\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e4e23b0d-4c70-4b5e-931a-34bd1499c952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image patches shape: (37, 49, 1, 256, 256, 3)\n",
      "Mask patches shape: (37, 49, 256, 256)\n"
     ]
    }
   ],
   "source": [
    "val_images, val_masks = process_and_save_images(image_paths_val, label_paths_val)\n",
    "val_dataset = CustomSegmentationDataset(val_images, val_masks, transform=val_transform)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=4, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7480b421-b56d-4d90-914a-9e7cbdadd10b",
   "metadata": {},
   "source": [
    "# Setting the Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54c85e1c-8260-4b43-be48-db458ad8708c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dinov2ForSemanticSegmentation(Dinov2PreTrainedModel):\n",
    "    def __init__(self, config, class_weights=None):\n",
    "        \"\"\"\n",
    "        Initialize the DINOv2 model for semantic segmentation.\n",
    "\n",
    "        Args:\n",
    "        - config: Configuration object containing model parameters.\n",
    "        - class_weights: Tensor containing class weights for the loss function.\n",
    "        \"\"\"\n",
    "        super().__init__(config)\n",
    "        self.dinov2 = Dinov2Model(config)  # Load the base DINOv2 model\n",
    "        self.classifier = LinearClassifier(config.hidden_size, 32, 32, config.num_labels)  # Define the classification head\n",
    "        self.class_weights = class_weights  # Store class weights\n",
    "\n",
    "    def forward(self, pixel_values, output_hidden_states=False, output_attentions=False, labels=None):\n",
    "        \"\"\"\n",
    "        Forward pass for the semantic segmentation model.\n",
    "\n",
    "        Args:\n",
    "        - pixel_values: Tensor of shape (batch_size, num_channels, height, width) containing input images.\n",
    "        - output_hidden_states: If True, return hidden states.\n",
    "        - output_attentions: If True, return attention weights.\n",
    "        - labels: Tensor of shape (batch_size, height, width) containing segmentation maps (optional).\n",
    "\n",
    "        Returns:\n",
    "        - SemanticSegmenterOutput containing:\n",
    "          - loss: Cross-entropy loss (if labels are provided).\n",
    "          - logits: Tensor of shape (batch_size, num_labels, height, width) containing class logits.\n",
    "          - hidden_states: List of hidden states from the DINOv2 model (if output_hidden_states is True).\n",
    "          - attentions: List of attention weights from the DINOv2 model (if output_attentions is True).\n",
    "        \"\"\"\n",
    "        # Obtain DINOv2 model outputs\n",
    "        outputs = self.dinov2(pixel_values, output_hidden_states=output_hidden_states, output_attentions=output_attentions)\n",
    "        \n",
    "        # Extract patch embeddings (excluding the CLS token)\n",
    "        patch_embeddings = outputs.last_hidden_state[:, 1:, :]\n",
    "        \n",
    "        # Check the shape of patch_embeddings\n",
    "        batch_size, num_patches, embedding_dim = patch_embeddings.shape\n",
    "        #print(f\"patch_embeddings shape: {patch_embeddings.shape}\")\n",
    "\n",
    "        # Calculate the expected number of patches\n",
    "        num_patches_expected = self.classifier.width * self.classifier.height\n",
    "        if num_patches != num_patches_expected:\n",
    "            raise ValueError(f\"Unexpected number of patches: {num_patches}, expected: {num_patches_expected}\")\n",
    "\n",
    "        # Apply the classification head to the patch embeddings\n",
    "        logits = self.classifier(patch_embeddings)\n",
    "        \n",
    "        # Resize logits to match the size of the input images\n",
    "        logits = torch.nn.functional.interpolate(logits, size=pixel_values.shape[2:], mode=\"bilinear\", align_corners=False)\n",
    "        \n",
    "        # Initialize loss to None\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # Compute the cross-entropy loss with class weights\n",
    "            #loss_fct = torch.nn.CrossEntropyLoss(weight=self.class_weights, ignore_index=0)  # Use 0 as the ignore index for background\n",
    "\n",
    "            loss_fct = torch.nn.CrossEntropyLoss(weight=self.class_weights)\n",
    "            loss = loss_fct(logits, labels)\n",
    "        \n",
    "        # Return the output containing the loss, logits, hidden states, and attentions\n",
    "        return SemanticSegmenterOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe46ffe9-a28f-4631-a7d3-87c1de06639d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearClassifier(torch.nn.Module):\n",
    "    def __init__(self, in_channels, tokenW=32, tokenH=32, num_labels=1):\n",
    "        super(LinearClassifier, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.width = tokenW\n",
    "        self.height = tokenH\n",
    "        self.classifier = torch.nn.Conv2d(in_channels, num_labels, (1,1))\n",
    "\n",
    "    def forward(self, embeddings):\n",
    "        embeddings = embeddings.reshape(-1, self.height, self.width, self.in_channels)\n",
    "        embeddings = embeddings.permute(0,3,1,2)\n",
    "\n",
    "        return self.classifier(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ffa6fdc3-9bfb-4239-957d-2f155f8f99b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1674681/3872288036.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load('/explore/nobackup/people/sking11/dinov2model_6400.pth')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dinov2ForSemanticSegmentation(\n",
       "  (dinov2): Dinov2Model(\n",
       "    (embeddings): Dinov2Embeddings(\n",
       "      (patch_embeddings): Dinov2PatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(14, 14), stride=(14, 14))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): Dinov2Encoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x Dinov2Layer(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attention): Dinov2Attention(\n",
       "            (attention): Dinov2SelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): Dinov2SelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (layer_scale1): Dinov2LayerScale()\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Dinov2MLP(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_scale2): Dinov2LayerScale()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "  )\n",
       "  (classifier): LinearClassifier(\n",
       "    (classifier): Conv2d(768, 2, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#set device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load the complete model\n",
    "model = torch.load('/explore/nobackup/people/sking11/dinov2model_6400.pth')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba9025e-bc87-4d75-bdd3-789c9b5933ff",
   "metadata": {},
   "source": [
    "# Doing Predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "893bb5d5-48b5-4211-a54c-82a37c236fba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1813 [00:00<?, ?it/s]/tmp/ipykernel_1674681/3060765076.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  pixel_values = torch.tensor(val_image).permute(0, 1, 2).unsqueeze(0).to(device)\n",
      "100%|██████████| 1813/1813 [01:16<00:00, 23.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All images processed and saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Directory to save predicted masks\n",
    "output_dir = \"/explore/nobackup/people/sking11/outputmasks\"\n",
    "\n",
    "# List to store all predicted masks\n",
    "predicted_masks_list = []\n",
    "\n",
    "# Ensure the model is in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Loop through each image in the validation dataset\n",
    "for idx in tqdm(range(len(val_dataset))):\n",
    "    val_image, _, original_image, _ = val_dataset[idx]\n",
    "    \n",
    "    # Convert the image to a tensor and add a batch dimension\n",
    "    pixel_values = torch.tensor(val_image).permute(0, 1, 2).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Forward pass through the model\n",
    "    with torch.no_grad():\n",
    "        outputs = model(pixel_values)\n",
    "    \n",
    "    # Upsample the logits to match the original image size\n",
    "    upsampled_logits = torch.nn.functional.interpolate(outputs.logits,\n",
    "                                                       size=original_image.shape[:2],\n",
    "                                                       mode=\"bilinear\", align_corners=False)\n",
    "    \n",
    "    # Get the predicted segmentation map\n",
    "    predicted_map = upsampled_logits.argmax(dim=1).squeeze().cpu().numpy()\n",
    "    \n",
    "    # Save the predicted map in the list\n",
    "    predicted_masks_list.append(predicted_map)\n",
    "    \n",
    "    # Convert predicted map to an image (you may need to normalize the values)\n",
    "    #predicted_image = Image.fromarray((predicted_map * 255).astype(np.uint8))\n",
    "    \n",
    "    # Save the predicted image\n",
    "    #output_path = os.path.join(output_dir, f\"predicted_mask_{idx}.png\")\n",
    "    #predicted_image.save(output_path)\n",
    "\n",
    "    #print(f\"Processed image {idx + 1}/{len(val_dataset)}\")\n",
    "\n",
    "print(\"All images processed and saved.\")\n",
    "\n",
    "# Now, `predicted_masks_list` contains all the predicted masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "859fc73c-1ae1-43b4-adc5-f2393bcb243c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1813\n",
      "1813\n"
     ]
    }
   ],
   "source": [
    "#Double check that lengths match up\n",
    "print(len(val_images))\n",
    "print(len(predicted_masks_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac0184b-f730-4697-b641-e1bb9c1b997e",
   "metadata": {},
   "source": [
    "Put all the mask pieces together into one complete image. The num_patches_x and num_patches_y should be changed to match the Image patches shape from the prior tiling step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bd89c266-8cd3-4d65-9519-d46aacf059b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "def unpatchify_binary(patches_list, patch_size, num_patches_x, num_patches_y):\n",
    "    \"\"\"\n",
    "    Reconstruct the full binary mask from a list of patches.\n",
    "    \n",
    "    Args:\n",
    "        patches_list (list of numpy.ndarray): List of patch arrays with binary values.\n",
    "        patch_size (tuple): Size of each patch (height, width).\n",
    "        num_patches_x (int): Number of patches along the height.\n",
    "        num_patches_y (int): Number of patches along the width.\n",
    "        \n",
    "    Returns:\n",
    "        numpy.ndarray: The reconstructed full-size binary mask.\n",
    "    \"\"\"\n",
    "    patch_height, patch_width = patch_size\n",
    "    full_height = num_patches_x * patch_height\n",
    "    full_width = num_patches_y * patch_width\n",
    "    \n",
    "    # Initialize the full-size mask array\n",
    "    full_mask = np.zeros((full_height, full_width), dtype=np.float32)\n",
    "    \n",
    "    # Initialize a count array to handle overlapping patches (if any)\n",
    "    count_array = np.zeros((full_height, full_width), dtype=np.float32)\n",
    "    \n",
    "    # Place each patch in the correct position\n",
    "    index = 0\n",
    "    for i in range(num_patches_x):\n",
    "        for j in range(num_patches_y):\n",
    "            if index >= len(patches_list):\n",
    "                print(f\"Warning: Expected more patches but reached the end of the list.\")\n",
    "                break\n",
    "            \n",
    "            patch = patches_list[index]\n",
    "            start_x = i * patch_height\n",
    "            start_y = j * patch_width\n",
    "            \n",
    "            full_mask[start_x:start_x+patch_height, start_y:start_y+patch_width] += patch\n",
    "            count_array[start_x:start_x+patch_height, start_y:start_y+patch_width] += 1\n",
    "            \n",
    "            index += 1\n",
    "    \n",
    "    # Normalize if any overlapping patches were averaged\n",
    "    full_mask = np.divide(full_mask, count_array, out=np.zeros_like(full_mask, dtype=np.float32), where=count_array!=0).astype(np.uint8)\n",
    "    \n",
    "    # Threshold to ensure binary output (0 or 1)\n",
    "    full_mask = np.clip(full_mask, 0, 1)\n",
    "\n",
    "    return full_mask\n",
    "\n",
    "# Example usage:\n",
    "num_patches_x = 37  # Number of rows\n",
    "num_patches_y = 49  # Number of columns\n",
    "patch_size = (256, 256)  # Height, width of each patch\n",
    "\n",
    "# Reconstruct the full binary mask from the list\n",
    "full_mask = unpatchify_binary(predicted_masks_list, patch_size, num_patches_x, num_patches_y)\n",
    "\n",
    "# Save or visualize the results\n",
    "# Convert binary mask to 'L' mode for saving\n",
    "Image.fromarray(full_mask * 255).convert('L').save('/explore/nobackup/people/sking11/fullmaskasdasd.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0ef307-93a5-452d-a7d3-754c4e3f0fa3",
   "metadata": {},
   "source": [
    "The mask won't line up over the original image as the patching process likely cropped it slightly, so the RGB image needs to be re-patched together as well. Then, you will have a complete predicted mask and an image to overlay it against.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e86d35-173d-4bea-87c7-ad29c9eb3f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpatchify_rgb(patches_list, patch_size, num_patches_x, num_patches_y):\n",
    "    \"\"\"\n",
    "    Reconstruct the full RGB image from a list of patches.\n",
    "    \n",
    "    Args:\n",
    "        patches_list (list of numpy.ndarray): List of patch arrays with RGB values.\n",
    "        patch_size (tuple): Size of each patch (height, width).\n",
    "        num_patches_x (int): Number of patches along the height.\n",
    "        num_patches_y (int): Number of patches along the width.\n",
    "        \n",
    "    Returns:\n",
    "        numpy.ndarray: The reconstructed full-size RGB image.\n",
    "    \"\"\"\n",
    "    patch_height, patch_width = patch_size\n",
    "    full_height = num_patches_x * patch_height\n",
    "    full_width = num_patches_y * patch_width\n",
    "    \n",
    "    # Initialize the full-size image array\n",
    "    full_image = np.zeros((full_height, full_width, 3), dtype=np.float32)\n",
    "    \n",
    "    # Initialize a count array to handle overlapping patches (if any)\n",
    "    count_array = np.zeros((full_height, full_width, 3), dtype=np.float32)\n",
    "    \n",
    "    # Place each patch in the correct position\n",
    "    index = 0\n",
    "    for i in range(num_patches_x):\n",
    "        for j in range(num_patches_y):\n",
    "            if index >= len(patches_list):\n",
    "                print(f\"Warning: Expected more patches but reached the end of the list.\")\n",
    "                break\n",
    "            \n",
    "            patch = patches_list[index]\n",
    "            start_x = i * patch_height\n",
    "            start_y = j * patch_width\n",
    "            \n",
    "            full_image[start_x:start_x+patch_height, start_y:start_y+patch_width, :] += patch\n",
    "            count_array[start_x:start_x+patch_height, start_y:start_y+patch_width, :] += 1\n",
    "            \n",
    "            index += 1\n",
    "    \n",
    "    # Normalize if any overlapping patches were averaged\n",
    "    full_image = np.divide(full_image, count_array, out=np.zeros_like(full_image, dtype=np.float32), where=count_array!=0).astype(np.uint8)\n",
    "    \n",
    "    return full_image\n",
    "\n",
    "# Example usage:\n",
    "num_patches_x = 37  # Number of rows\n",
    "num_patches_y = 49  # Number of columns\n",
    "patch_size = (256, 256)  # Height, width of each patch\n",
    "\n",
    "# Reconstruct the full RGB image from the list\n",
    "full_image = unpatchify_rgb(val_images, patch_size, num_patches_x, num_patches_y)\n",
    "\n",
    "# Save or visualize the results\n",
    "# Convert RGB image for saving\n",
    "Image.fromarray(full_image).save('/explore/nobackup/people/sking11/full_imageasdasd.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dino_env",
   "language": "python",
   "name": "dino_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
